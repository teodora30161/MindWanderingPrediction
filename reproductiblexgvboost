import pandas as pd
import numpy as np
import optuna
import matplotlib.pyplot as plt
import seaborn as sns
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import LeaveOneGroupOut, train_test_split
from sklearn.metrics import balanced_accuracy_score, roc_curve, auc, precision_recall_curve
import os
from datetime import datetime
import joblib
import random
import pickle

class MindWanderingClassifier:
    """Class for mind wandering classification using XGBoost."""
    
    def __init__(self, random_state=42, results_dir="results"):
        """Initialize the classifier with parameters and directories."""
        self.random_state = random_state
        # Set all random seeds at initialization
        np.random.seed(self.random_state)
        random.seed(self.random_state)
        # Note: Optuna seed is set during optimization
        
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.results_dir = results_dir
        os.makedirs(self.results_dir, exist_ok=True)
        
        # Initialize models and metrics containers
        self.best_model_orig = None
        self.best_model_smote = None
        self.features = None
        self.target = None
        self.engineered_features = []
        
        # For saving/loading train-test splits
        self.split_path = os.path.join(self.results_dir, "train_test_split.pkl")
        
        # For saving/loading hyperparameters
        self.hyperparams_orig_path = os.path.join(self.results_dir, "best_hyperparams_orig.pkl")
        self.hyperparams_smote_path = os.path.join(self.results_dir, "best_hyperparams_smote.pkl")
        
        # Performance metrics
        self.balanced_acc_orig = None
        self.balanced_acc_smote = None
        self.roc_auc_orig = None
        self.roc_auc_smote = None
        
    def load_data(self, file_path, target='ON_OFF', group_column='subj'):
        """Load and prepare the dataset."""
        print(f"Loading data from {file_path}...")
        self.df = pd.read_csv(file_path)
        self.target = target
        self.group_column = group_column
        self.total_samples = len(self.df)
        print(f"Loaded {self.total_samples} records.")
        return self
        
    def select_features(self, features=None):
        """Select features for the model."""
        if features is None:
            features = ['BV', 'AE', 'pupil_size_median']
        self.features = features
        print(f"Selected features: {self.features}")
        return self
        
    def engineer_features(self):
        """Create additional features from existing ones."""
        print("Engineering additional features...")
        
        if 'BV' in self.features and 'AE' in self.features:
            self.df['BV_AE'] = self.df['BV'] * self.df['AE']
            self.engineered_features.append('BV_AE')
            
        if 'BV' in self.features and 'pupil_size_median' in self.features:
            self.df['BV_ratio_pupil'] = self.df['BV'] / (self.df['pupil_size_median'] + 1e-6)
            self.engineered_features.append('BV_ratio_pupil')
            
        if 'AE' in self.features and 'pupil_size_median' in self.features:
            self.df['AE_pupil'] = self.df['AE'] / (self.df['pupil_size_median'] + 1e-6)
            self.engineered_features.append('AE_pupil')
        
        self.features = self.features + self.engineered_features
        print(f"Added engineered features: {self.engineered_features}")
        return self
        
    def clean_data(self):
        """Clean data by removing NaNs and encoding target variable."""
        print("Cleaning data...")
        all_needed_cols = self.features + [self.target, self.group_column]
        self.df_clean = self.df.dropna(subset=all_needed_cols).copy()
        self.clean_samples = len(self.df_clean)
        
        if self.df_clean[self.target].dtype == 'object':
            print(f"Encoding target variable '{self.target}'...")
            self.df_clean[self.target] = self.df_clean[self.target].map({'ON': 0, 'OFF': 1})
        
        print(f"Clean data shape: {self.df_clean.shape}")
        return self
        
    def prepare_datasets(self, test_size=0.2, use_saved_split=True):
        """Prepare train and test datasets with option to use saved splits."""
        print("Preparing datasets...")
        self.X = self.df_clean[self.features].values
        self.y = self.df_clean[self.target].values
        self.groups = self.df_clean[self.group_column].values
        
        # Check if we should use a saved split
        if use_saved_split and os.path.exists(self.split_path):
            print(f"Loading existing train-test split from {self.split_path}")
            with open(self.split_path, 'rb') as f:
                split_indices = pickle.load(f)
                train_indices = split_indices['train_indices']
                test_indices = split_indices['test_indices']
                
            self.X_train_orig = self.X[train_indices]
            self.X_test_orig = self.X[test_indices]
            self.y_train_orig = self.y[train_indices]
            self.y_test_orig = self.y[test_indices]
        else:
            # Create new train-test split
            print("Creating new train-test split")
            self.X_train_orig, self.X_test_orig, self.y_train_orig, self.y_test_orig, train_indices, test_indices = train_test_split(
                self.X, self.y, np.arange(len(self.X)), 
                test_size=test_size, 
                random_state=self.random_state, 
                stratify=self.y
            )
            
            # Save the split indices for future use
            with open(self.split_path, 'wb') as f:
                pickle.dump({
                    'train_indices': train_indices, 
                    'test_indices': test_indices
                }, f)
            print(f"Saved train-test split to {self.split_path}")
        
        # Store class distribution
        self.y_train_orig_dist = np.bincount(self.y_train_orig)
        self.y_test_orig_dist = np.bincount(self.y_test_orig)
        
        # Apply SMOTE with fixed random state
        smote = SMOTE(sampling_strategy=0.5, random_state=self.random_state)
        self.X_train_smote, self.y_train_smote = smote.fit_resample(self.X_train_orig, self.y_train_orig)
        
        # Store SMOTE class distribution
        self.y_train_smote_dist = np.bincount(self.y_train_smote)
        
        print(f"Original training set shape: {self.X_train_orig.shape}")
        print(f"SMOTE-resampled training set shape: {self.X_train_smote.shape}")
        print(f"Test set shape: {self.X_test_orig.shape}")
        
        return self
        
    def optimize_hyperparameters(self, n_trials=50, use_saved_params=True):
        """Optimize hyperparameters using Optuna with option to use saved parameters."""
        # Optimize for original dataset
        self._optimize_hyperparameters_for_dataset(
            "original", 
            self.X_train_orig, 
            self.y_train_orig,
            n_trials, 
            use_saved_params,
            self.hyperparams_orig_path
        )
        
        # Optimize for SMOTE dataset
        self._optimize_hyperparameters_for_dataset(
            "SMOTE", 
            self.X_train_smote, 
            self.y_train_smote,
            n_trials, 
            use_saved_params,
            self.hyperparams_smote_path
        )
        
        return self
    
    def _optimize_hyperparameters_for_dataset(self, dataset_name, X_train, y_train, n_trials, use_saved_params, params_path):
        """Helper method to optimize hyperparameters for a specific dataset."""
        # Check if we should use saved parameters
        if use_saved_params and os.path.exists(params_path):
            print(f"Loading existing hyperparameters for {dataset_name} dataset from {params_path}")
            with open(params_path, 'rb') as f:
                best_params = pickle.load(f)
                
            # Create a dummy study to store the parameters
            study = optuna.create_study(direction='maximize')
            study.set_user_attr("best_params", best_params) 

            
            if dataset_name == "original":
                self.study_orig = study
            else:
                self.study_smote = study
                
            print(f"Loaded best parameters for {dataset_name} dataset: {best_params}")
            return
        
        print(f"Optimizing hyperparameters for {dataset_name} dataset with {n_trials} trials...")
        
        # Set the Optuna seed
        optuna.logging.set_verbosity(optuna.logging.WARNING)
        
        def objective(trial, X_tr, y_tr, X_te, y_te):
            params = {
                'max_depth': trial.suggest_int('max_depth', 3, 10),
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
                'gamma': trial.suggest_float('gamma', 0.01, 1.0),
                'subsample': trial.suggest_float('subsample', 0.5, 1.0),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
                'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1.0, 5.0),
                'random_state': self.random_state,  # Ensure XGBoost randomness is controlled
            }
            model = XGBClassifier(**params, use_label_encoder=False, eval_metric='logloss')
            model.fit(X_tr, y_tr)
            y_pred = model.predict(X_te)
            return balanced_accuracy_score(y_te, y_pred)

        # Set a seed for the sampler to ensure reproducible optimization
        sampler = optuna.samplers.TPESampler(seed=self.random_state)
        study = optuna.create_study(direction='maximize', sampler=sampler)
        study.optimize(
            lambda trial: objective(trial, X_train, y_train, self.X_test_orig, self.y_test_orig), 
            n_trials=n_trials
        )
        
        # Save the best parameters
        with open(params_path, 'wb') as f:
            pickle.dump(study.best_params, f)
        print(f"Saved best parameters for {dataset_name} dataset to {params_path}: {study.best_params}")
        
        # Store the study
        if dataset_name == "original":
            self.study_orig = study
        else:
            self.study_smote = study
        
    def train_models(self):
        """Train models with optimized hyperparameters."""
        print("Training final models...")
        
        # Train model with original data
        model_params_orig = self.study_orig.best_params.copy()
        model_params_orig['random_state'] = self.random_state
        
        self.best_model_orig = XGBClassifier(**model_params_orig, 
                                           use_label_encoder=False, 
                                           eval_metric='logloss')
        self.best_model_orig.fit(self.X_train_orig, self.y_train_orig)
        
        # Train model with SMOTE data
        model_params_smote = self.study_smote.best_params.copy()
        model_params_smote['random_state'] = self.random_state
        
        self.best_model_smote = XGBClassifier(**model_params_smote, 
                                            use_label_encoder=False, 
                                            eval_metric='logloss')
        self.best_model_smote.fit(self.X_train_smote, self.y_train_smote)
        
        # Evaluate models
        y_pred_orig = self.best_model_orig.predict(self.X_test_orig)
        y_pred_smote = self.best_model_smote.predict(self.X_test_orig)
        
        # Calculate balanced accuracy
        self.balanced_acc_orig = balanced_accuracy_score(self.y_test_orig, y_pred_orig)
        self.balanced_acc_smote = balanced_accuracy_score(self.y_test_orig, y_pred_smote)
        
        # Calculate ROC AUC
        y_proba_orig = self.best_model_orig.predict_proba(self.X_test_orig)[:, 1]
        y_proba_smote = self.best_model_smote.predict_proba(self.X_test_orig)[:, 1]
        
        fpr_orig, tpr_orig, _ = roc_curve(self.y_test_orig, y_proba_orig)
        fpr_smote, tpr_smote, _ = roc_curve(self.y_test_orig, y_proba_smote)
        
        self.roc_auc_orig = auc(fpr_orig, tpr_orig)
        self.roc_auc_smote = auc(fpr_smote, tpr_smote)
        
        print(f"Original model - Balanced Accuracy: {self.balanced_acc_orig:.4f}, ROC AUC: {self.roc_auc_orig:.4f}")
        print(f"SMOTE model - Balanced Accuracy: {self.balanced_acc_smote:.4f}, ROC AUC: {self.roc_auc_smote:.4f}")
        
        return self
        
    def perform_loso_cv(self):
        """Perform Leave-One-Subject-Out cross-validation."""
        print("Performing Leave-One-Subject-Out cross-validation...")
        logo = LeaveOneGroupOut()
        self.participant_aucs = {}
        self.y_test_pooled, self.y_probs_pooled = [], []
        
        # Copy best parameters and add random_state
        best_model_params = self.study_orig.best_params.copy()
        best_model_params['random_state'] = self.random_state
        
        for train_idx, test_idx in logo.split(self.X, self.y, self.groups):
            participant = np.unique(self.groups[test_idx])[0]
            print(f"Processing Participant: {participant}")
            
            X_train_fold, X_test_fold = self.X[train_idx], self.X[test_idx]
            y_train_fold, y_test_fold = self.y[train_idx], self.y[test_idx]
            
            fold_model = XGBClassifier(**best_model_params, use_label_encoder=False, eval_metric='logloss')
            fold_model.fit(X_train_fold, y_train_fold)
            
            y_probs_fold = fold_model.predict_proba(X_test_fold)[:, 1]
            fpr, tpr, _ = roc_curve(y_test_fold, y_probs_fold)
            fold_auc = auc(fpr, tpr)
                
            self.participant_aucs[participant] = fold_auc
            self.y_test_pooled.extend(y_test_fold)
            self.y_probs_pooled.extend(y_probs_fold)
        
        fpr_pooled, tpr_pooled, _ = roc_curve(self.y_test_pooled, self.y_probs_pooled)
        self.roc_auc_pooled = auc(fpr_pooled, tpr_pooled)
        
        print(f"Final LOSO-Based AUC using Pooled Test Set: {self.roc_auc_pooled:.4f}")
        return self
        
    def plot_feature_importance(self):
        """Plot feature importance from the best model."""
        print("Plotting feature importance...")
        
        # Get feature importance
        importance = self.best_model_orig.feature_importances_
        
        # Create a DataFrame for better visualization
        feature_importance = pd.DataFrame({
            'Feature': self.features,
            'Importance': importance
        })
        feature_importance = feature_importance.sort_values('Importance', ascending=False)
        
        # Plot feature importance
        plt.figure(figsize=(12, 8))
        sns.barplot(x='Importance', y='Feature', data=feature_importance)
        plt.title('Feature Importance', fontsize=16)
        plt.tight_layout()
        
        # Save the plot
        plt.savefig(os.path.join(self.results_dir, f"feature_importance_{self.timestamp}.png"), dpi=300)
        plt.close()
        
        return self
        
    def plot_loso_results(self):
        """Plot LOSO cross-validation results."""
        print("Plotting LOSO cross-validation results...")
        
        # Plot individual participant AUCs
        plt.figure(figsize=(14, 6))
        participants = list(self.participant_aucs.keys())
        aucs = list(self.participant_aucs.values())
        
        # Sort by AUC value
        sorted_indices = np.argsort(aucs)
        sorted_participants = [participants[i] for i in sorted_indices]
        sorted_aucs = [aucs[i] for i in sorted_indices]
        
        # Plot
        bars = plt.bar(sorted_participants, sorted_aucs, color='skyblue')
        
        # Add mean line
        plt.axhline(y=np.mean(aucs), color='red', linestyle='-', label=f'Mean AUC: {np.mean(aucs):.3f}')
        
        # Add labels and title
        plt.xlabel('Participant', fontsize=14)
        plt.ylabel('AUC', fontsize=14)
        plt.title('AUC per Participant (LOSO Cross-Validation)', fontsize=16)
        plt.xticks(rotation=90)
        plt.legend()
        plt.tight_layout()
        
        # Save the plot
        plt.savefig(os.path.join(self.results_dir, f"loso_auc_per_participant_{self.timestamp}.png"), dpi=300)
        plt.close()
        
        return self
        
    def plot_roc_curves(self):
        """Plot ROC curves for the model."""
        print("Plotting ROC curves...")
        
        # Set matplotlib seed for consistent plotting
        np.random.seed(self.random_state)
        
        # Calculate ROC curve for the pooled test set
        fpr_pooled, tpr_pooled, _ = roc_curve(self.y_test_pooled, self.y_probs_pooled)
        
        # Plot ROC curve
        plt.figure(figsize=(10, 8))
        plt.plot(fpr_pooled, tpr_pooled, 'b-', linewidth=2, 
                 label=f'ROC Curve (AUC = {self.roc_auc_pooled:.3f})')
        
        # Add reference line for random classifier
        plt.plot([0, 1], [0, 1], 'k--', linewidth=1)
        
        # Add labels and title
        plt.xlabel('False Positive Rate', fontsize=14)
        plt.ylabel('True Positive Rate', fontsize=14)
        plt.title('ROC Curve (Pooled Test Set)', fontsize=16)
        plt.legend(loc='lower right')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        # Save the plot
        plt.savefig(os.path.join(self.results_dir, f"roc_curve_pooled_{self.timestamp}.png"), dpi=300)
        plt.close()
        
        return self
        
    def plot_roc_comparison(self):
        print("Plotting ROC curve comparison...")
    
        # Set matplotlib seed for consistent plotting
        np.random.seed(self.random_state)
    
        # Calculate ROC curves for both models
        y_proba_orig = self.best_model_orig.predict_proba(self.X_test_orig)[:, 1]
        y_proba_smote = self.best_model_smote.predict_proba(self.X_test_orig)[:, 1]
    
        fpr_orig, tpr_orig, _ = roc_curve(self.y_test_orig, y_proba_orig)
        fpr_smote, tpr_smote, _ = roc_curve(self.y_test_orig, y_proba_smote)
        
        # Plot ROC curves
        plt.figure(figsize=(10, 8))
        plt.plot(fpr_orig, tpr_orig, 'b-', linewidth=2, 
                label=f'Original Model (AUC = {self.roc_auc_orig:.3f})')
        plt.plot(fpr_smote, tpr_smote, 'r-', linewidth=2, 
                label=f'SMOTE Model (AUC = {self.roc_auc_smote:.3f})')
        
        # Add reference line for random classifier
        plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')
        
        # Add labels and title
        plt.xlabel('False Positive Rate', fontsize=14)
        plt.ylabel('True Positive Rate', fontsize=14)
        plt.title('ROC Curve Comparison: Original vs SMOTE Model', fontsize=16)
        plt.legend(loc='lower right')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        # Save the plot
        plt.savefig(os.path.join(self.results_dir, f"roc_comparison_{self.timestamp}.png"), dpi=300)
        plt.close()
        
        return self
        
    def generate_report(self):
        """Generate a comprehensive classification report."""
        print("Generating classification report...")
        
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        report_path = os.path.join(self.results_dir, f"classification_report_{self.timestamp}.txt")
        
        with open(report_path, 'w') as f:
            f.write("==== MIND WANDERING CLASSIFICATION REPORT ====\n")
            f.write(f"  Date: {current_time}\n\n")
            
            # Data summary
            f.write("== DATA SUMMARY ==\n")
            f.write(f"Total samples: {self.total_samples}\n")
            f.write(f"Clean samples: {self.clean_samples}\n")
            f.write(f"Features used: {', '.join(self.features)}\n")
            f.write(f"Target variable: {self.target}\n")
            f.write(f"Group variable: {self.group_column}\n\n")
            
            # Class distribution
            f.write("== CLASS DISTRIBUTION ==\n")
            f.write(f"Original training set: {self.y_train_orig_dist}\n")
            f.write(f"SMOTE-resampled training set: {self.y_train_smote_dist}\n")
            f.write(f"Test set: {self.y_test_orig_dist}\n\n")
            
            # Model performance
            f.write("== MODEL PERFORMANCE ==\n")
            f.write(f"Original model - Balanced Accuracy: {self.balanced_acc_orig:.4f}, ROC AUC: {self.roc_auc_orig:.4f}\n")
            f.write(f"SMOTE model - Balanced Accuracy: {self.balanced_acc_smote:.4f}, ROC AUC: {self.roc_auc_smote:.4f}\n\n")
            
            # LOSO results
            f.write("== LEAVE-ONE-SUBJECT-OUT RESULTS ==\n")
            f.write("AUC per Participant:\n")
            for participant, participant_auc in self.participant_aucs.items():
                f.write(f"{participant}: {participant_auc:.4f}\n")
            f.write(f"\nLOSO Pooled AUC: {self.roc_auc_pooled:.4f}\n\n")
            
            # Best hyperparameters
            f.write("== BEST HYPERPARAMETERS ==\n")
            f.write("Original model:\n")
            for param, value in self.study_orig.best_params.items():
                f.write(f"  {param}: {value}\n")
            
            f.write("\nSMOTE model:\n")
            for param, value in self.study_smote.best_params.items():
                f.write(f"  {param}: {value}\n")
        
        print(f"Report saved to {report_path}")
        return self
        
    def save_results(self):
        """Save model and results to disk."""
        print("Saving results...")
        
        # Create a results dictionary
        results = {
            'best_model_params_orig': self.study_orig.best_params,
            'best_model_params_smote': self.study_smote.best_params,
            'selected_features': self.features,
            'engineered_features': self.engineered_features,
            'participant_aucs': self.participant_aucs,
            'mean_auc': np.mean(list(self.participant_aucs.values())),
            'pooled_auc': self.roc_auc_pooled,
            'balanced_acc_orig': self.balanced_acc_orig,
            'balanced_acc_smote': self.balanced_acc_smote,
            'roc_auc_orig': self.roc_auc_orig,
            'roc_auc_smote': self.roc_auc_smote,
            'timestamp': self.timestamp,
            'random_state': self.random_state
        }
        
        # Save the models
        orig_model_path = os.path.join(self.results_dir, f"best_model_orig_{self.timestamp}.joblib")
        smote_model_path = os.path.join(self.results_dir, f"best_model_smote_{self.timestamp}.joblib")
        joblib.dump(self.best_model_orig, orig_model_path)
        joblib.dump(self.best_model_smote, smote_model_path)
        
        # Save the results
        results_path = os.path.join(self.results_dir, f"results_{self.timestamp}.joblib")
        joblib.dump(results, results_path)
        
        print(f"Results saved to {self.results_dir}")
        return self
    
    def run_pipeline(self, file_path, features=None, n_trials=50, use_saved_split=True, use_saved_params=True):
        """Run the entire pipeline in one method."""
        return (self
            .load_data(file_path)
            .select_features(features)
            .engineer_features()
            .clean_data()
            .prepare_datasets(use_saved_split=use_saved_split)
            .optimize_hyperparameters(n_trials=n_trials, use_saved_params=use_saved_params)
            .train_models()
            .perform_loso_cv()
            .plot_feature_importance()
            .plot_loso_results()
            .plot_roc_curves()
            .plot_roc_comparison()
            .generate_report()
            .save_results()
        )

# Main execution
if __name__ == "__main__":
    # Set global random seeds for reproducibility
    RANDOM_SEED = 42
    random.seed(RANDOM_SEED)
    np.random.seed(RANDOM_SEED)
    
    file_path = "/Users/teostei/Desktop/MINDWANDERINGDEEPLEARNING/data_theodora/bhv/2_clean/all_probes4.csv"
    
    classifier = MindWanderingClassifier(random_state=RANDOM_SEED, results_dir="mind_wanderingML_results")
    classifier.run_pipeline(
        file_path=file_path,
        features=None,  # Use defaults
        n_trials=50,
        use_saved_split=True,
        use_saved_params=True
    )

    print("Analysis complete!")